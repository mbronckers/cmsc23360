{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.2\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-692932d74be4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os \n",
    "from joblib import dump, load\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis on collected local data \n",
    "\n",
    "In this section, we wanted to investigate what type of model (Random Forest, SVM, or Lostical Regression) would be the most promising. Additionally, we also wanted to find out what packet protocols should we train on given the relaxed conditions. This section does relate to the Bonus task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF acc: 0.19894230579312253\n",
      "{'all': {'rf': '0.19894230579312253'}}\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "path = '../../p2/collection'\n",
    "col_names = [\"website-index\", \"time\", \"direction\", \"packet size\"]\n",
    "folders = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for folder in d:\n",
    "        folders.append(folder)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop over the folders in /collection\n",
    "for folder in folders:\n",
    "    if (folder == \"raw\"):\n",
    "        continue \n",
    "    results[folder] = {} # Goddam python dictionaries\n",
    "\n",
    "    # Load data into dataframes\n",
    "    df_one = pd.read_csv(path+\"/\"+folder+\"/\"+\"Day1-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "    df_two = pd.read_csv(path+\"/\"+folder+\"/\"+\"Day2-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "    df_three = pd.read_csv(path+\"/\"+folder+\"/\"+\"Day3-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "    \n",
    "    # Preprocess training data\n",
    "    df = df_one.append(df_two)\n",
    "    X = df.loc[:, df.columns != 'website-index']\n",
    "    y = df['website-index']\n",
    "    \n",
    "    # Preprocess testing data\n",
    "    X_test = df_three.loc[:, df_three.columns != 'website-index']\n",
    "    y_test = df_three['website-index']\n",
    "    \n",
    "    outpath = './models/' + folder + \"/\"\n",
    "    if not os.path.exists(outpath):\n",
    "        os.mkdir(outpath)    \n",
    "\n",
    "    # RF model training\n",
    "    clf = RandomForestClassifier(n_jobs=-1)\n",
    "    clf.fit(X,y)\n",
    "    rf_acc = clf.score(X_test, y_test)\n",
    "    print(\"RF acc: \" + str(rf_acc))\n",
    "    results[folder][\"rf\"] = str(rf_acc)\n",
    "    dump(clf, outpath + 'rf_model.joblib')\n",
    "    print(results)\n",
    "\n",
    "    # SVM model training\n",
    "    svm_model = svm.SVC(verbose=True, cache_size=8000) # Yes i have the ram\n",
    "    svm_model.fit(X, y)\n",
    "    svm_acc = svm_model.score(X_test, y_test)\n",
    "    print(\"SVM acc: \" + str(svm_acc))\n",
    "    results[folder][\"svm\"] = str(svm_acc)\n",
    "    dump(svm_model, outpath + 'svm_model.joblib')\n",
    "    print(results)\n",
    "\n",
    "    # LR model training\n",
    "    lr = LogisticRegression(random_state=0, max_iter=10e5, n_jobs=-1)\n",
    "    lr.fit(X, y)\n",
    "    lr_acc = lr.score(X_test, y_test)\n",
    "    print(\"Logistic Reg acc: \" + str(lr_acc))\n",
    "    results[folder][\"lr\"] = str(lr_acc)\n",
    "    dump(lr, outpath + \"lr_model.joblib\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these preliminary results, we then plotted the training accuracies for each model type (SVM, RF, LR), for each combination of protocols (HTTP, UDP, TCP, TLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_key = svm_key = list(map(lambda x : x[0], svm_results))\n",
    "x_pos = [i for i, _ in enumerate(x_key)]\n",
    "\n",
    "svm_results = [(k, r['svm']) for k,r in results.items()]\n",
    "svm_val = list(map(lambda x : float(x[1]), svm_results))\n",
    "\n",
    "rf_results = [(k, r['rf']) for k,r in results.items()]\n",
    "rf_val = list(map(lambda x : float(x[1]), rf_results))\n",
    "\n",
    "lr_results =  [(k, r['lr']) for k,r in results.items()]\n",
    "lr_val = list(map(lambda x : float(x[1]), lr_results))\n",
    "\n",
    "# multiple line plot\n",
    "plt.plot(x_pos, svm_val, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2, label='SVM acc')\n",
    "plt.plot(x_pos, rf_val, marker='o', markerfacecolor='red', markersize=12, color='olive', linewidth=2, label='RF acc')\n",
    "plt.plot(x_pos, lr_val, marker='o', markerfacecolor='green', markersize=12, color='yellow', linewidth=2, label='LR acc')\n",
    "plt.xticks(x_pos, x_key, rotation='vertical')\n",
    "plt.xlabel(\"protocol\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model accuracy of each protocol type on each model\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we also wanted to see what the average accuracy of the 3 model types are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_av = np.mean(lr_val)\n",
    "rf_av = np.mean(rf_val)\n",
    "svm_av = np.mean(svm_val)\n",
    "plt.bar([0,1,2], [lr_av, rf_av, svm_av])\n",
    "plt.xticks([0,1,2], ['lr', 'rf', 'svm'])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_av = np.mean(lr_val)\n",
    "rf_av = np.mean(rf_val)\n",
    "svm_av = np.mean(svm_val)\n",
    "plt.bar([0,1,2], [lr_av, rf_av, svm_av])\n",
    "plt.xticks([0,1,2], ['lr', 'rf', 'svm'])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average accuracy\")\n",
    "plt.title(\"Average accuracy of the 3 model types\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, using UDP with Logistical Regression (LR) seems to provide the best results. \n",
    "\n",
    "However, averaging out across protocols, LR performs the worst. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to verify the impact on accuracy depending on the protocol used. \n",
    "\n",
    "To do this we calculate the average accuracies for each model type (lr, rf, svm) for each model that uses http, udp, tls, tcp protocols. \n",
    "\n",
    "For example, we calculate the average accuracy given model type SVM that uses UDP packets (this includes the following data sets: UDP, HTTP + UDP, TCP + UDP, UDP + TCO + TLS, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HTTP, TCP, UDP, TLS, plot their average accuracy on each of the 3 models\n",
    "# x-axis: protcols that uses http, tcp, udp, or tls\n",
    "# y-axis average accuracy on for models that used that protocol\n",
    "http = [d for d in x_key if d.find(\"http\") >= 0]\n",
    "udp = [d for d in x_key if d.find(\"udp\") >= 0]\n",
    "tls = [d for d in x_key if d.find(\"tls\") >= 0]\n",
    "tcp = [d for d in x_key if d.find(\"tcp\") >= 0]\n",
    "\n",
    "# print(http)\n",
    "# print(udp)\n",
    "# print(tls)\n",
    "# print(tcp)\n",
    "\n",
    "http_svm_val = [r['svm'] for k,r in results.items() if k in http]\n",
    "udp_svm_val = [r['svm'] for k,r in results.items() if k in udp]\n",
    "tls_svm_val = [r['svm'] for k,r in results.items() if k in tls]\n",
    "tcp_svm_val = [r['svm'] for k,r in results.items() if k in tcp]\n",
    "\n",
    "http_lr_val = [r['lr'] for k,r in results.items() if k in http]\n",
    "udp_lr_val = [r['lr'] for k,r in results.items() if k in udp]\n",
    "tls_lr_val = [r['lr'] for k,r in results.items() if k in tls]\n",
    "tcp_lr_val = [r['lr'] for k,r in results.items() if k in tcp]\n",
    "\n",
    "http_rf_val = [r['rf'] for k,r in results.items() if k in http]\n",
    "udp_rf_val = [r['rf'] for k,r in results.items() if k in udp]\n",
    "tls_rf_val = [r['rf'] for k,r in results.items() if k in tls]\n",
    "tcp_rf_val = [r['rf'] for k,r in results.items() if k in tcp]\n",
    "\n",
    "http_svm_av = np.mean(list(map(lambda x: float(x), http_svm_val)))\n",
    "http_lr_av = np.mean(list(map(lambda x: float(x), http_lr_val)))\n",
    "http_rf_av = np.mean(list(map(lambda x: float(x), http_rf_val)))\n",
    "\n",
    "udp_svm_av = np.mean(list(map(lambda x: float(x), udp_svm_val)))\n",
    "udp_lr_av = np.mean(list(map(lambda x: float(x), udp_lr_val)))\n",
    "udp_rf_av = np.mean(list(map(lambda x: float(x), udp_rf_val)))\n",
    "\n",
    "tls_svm_av = np.mean(list(map(lambda x: float(x), tls_svm_val)))\n",
    "tls_lr_av = np.mean(list(map(lambda x: float(x), tls_lr_val)))\n",
    "tls_rf_av = np.mean(list(map(lambda x: float(x), tls_rf_val)))\n",
    "\n",
    "tcp_svm_av = np.mean(list(map(lambda x: float(x), tcp_svm_val)))\n",
    "tcp_lr_av = np.mean(list(map(lambda x: float(x), tcp_lr_val)))\n",
    "tcp_rf_av = np.mean(list(map(lambda x: float(x), tcp_rf_val)))\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"\",\"SVM\",\"LR\", \"RF\"],\n",
    "         [\"HTTP\", http_svm_av, http_lr_av, http_rf_av],\n",
    "         [\"UDP\", udp_svm_av, udp_lr_av, udp_rf_av],\n",
    "         [\"TLS\", tls_svm_av, tls_lr_av, tls_rf_av],\n",
    "         [\"TCP\",tcp_svm_av, tcp_lr_av, tcp_rf_av]\n",
    "        ]\n",
    "\n",
    "print(\"Average accuracy of protocol usage against model\")\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html', floatfmt=(\".4f\", \".4f\", \".4f\"))))\n",
    "\n",
    "# We would also want to normalize it by the average proportion of protocol packtes in each dataset that the protocol was part of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, it seems like HTTP and UDP plays a big role in SVM. \n",
    "\n",
    "However, findings in this table must be taken with a grain of salt as protocol usage is very skewed in the data i.e. a data collection might have 4 HTTP packets and hundreds of UDP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, UDP seems to be the most promising. We theorize that TCP data has too much noise where a lot of different services are also using TCP. Additionally, we guess that most of the UDP traffic are responsible by websites. Hence manual selection of the UDP protocol data provides the best results as it minimizes other noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local data + Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we decided to use HTTP + UDP packets for our local data training, we import them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data\n",
    "path = \"collection/\"\n",
    "folder = \"df_http_udp/\" # specify which packet protocol config you want here\n",
    "col_names = [\"website-index\", \"time\", \"direction\", \"packet size\"]\n",
    "df_one_local = pd.read_csv(path+folder+\"Day1-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "df_two_local = pd.read_csv(path+folder+\"Day2-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "df_three_local = pd.read_csv(path+folder+\"Day3-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceeded with feature engineering. We chose the following features: \n",
    "\n",
    "- Cumulative representation of trace\n",
    "- Cumulative Counting Statistics\n",
    "    - Counting cumulative packets\n",
    "    - Counting incoming cmulative packets \n",
    "    - Counting outgoing cumulative packets \n",
    "    - Counting cumulative percentage of outgoing/incoming packets \n",
    "- Previous 100 packet sizes\n",
    "- Total Counting statistics\n",
    "- Packet ordering statistics \n",
    "- Concentration of incoming & outgoing packets in first and last 30 packets \n",
    "- Concentration of outgoing packets \n",
    "\n",
    "All of the code for feature engineering can be found at the bottom of this document, along with their descriptions.\n",
    "\n",
    "We do howerver, include a plot here of cumulative model accuracy (Random Forest) for each feature mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_key = ['cumulative representation', 'cumulative counting statistics', 'previous 100 packet sizes', 'total counting statistics', 'packet ordering statistics', 'concentration of incoming/outgoing in first/last 30', 'concentration of outgoing packets']\n",
    "x_pos = [i for i, _ in enumerate(x_key)]\n",
    "y_val = [0.7765592186406214, 0.7963265442181406, 0.813893796459882, 0.8336277875929198, 0.8277609253641788, 0.8339611320377346, 0.8328277609253641]\n",
    "\n",
    "# multiple line plot\n",
    "plt.plot(x_pos, y_val, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2, label='SVM acc')\n",
    "\n",
    "plt.xticks(x_pos, x_key, rotation='vertical')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Cumulative Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing local data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_stat = outgoing_concentration(df_one, prev=True)\n",
    "df_two_stat = outgoing_concentration(df_two, prev=True)\n",
    "df_three_stat = outgoing_concentration(df_three, prev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panchenko et al [fingerprinting-ndss2016]; cumulative representation of trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_packet_size(x):\n",
    "    if (x['direction'] == 1):\n",
    "        return -x['packet size']\n",
    "    else:\n",
    "        return x['packet size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cum_trace(df):    \n",
    "    # Create a directional packet size column p_i (negative values if outbound)\n",
    "    df.insert(4, 'p_i', df['packet size'])\n",
    "    df['p_i'] = df.apply(directional_packet_size, axis=1)\n",
    "\n",
    "    # Create cumulative packet size trace using directional packet sizes\n",
    "    df['c_i'] = df.groupby('website-index')['p_i'].cumsum(axis=None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics features per Hayes et al [sec16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_packets(df, trace=False):\n",
    "    # also use ndss2016\n",
    "    if (trace):\n",
    "        df = cum_trace(df)\n",
    "        \n",
    "    # cumulative packets counter\n",
    "    df['cum_count'] = df.groupby('website-index').cumcount()+1\n",
    "    \n",
    "    # cumulative incoming packets counter\n",
    "    df['cum_inc'] = df.groupby('website-index')['direction'].cumsum(axis=None)\n",
    "    \n",
    "    # cumulative outgoing packets counter\n",
    "    df['cum_out'] = df['cum_count'] - df['cum_inc']\n",
    "    \n",
    "    # cumulative percentage outgoing/incoming\n",
    "    df['perc_in'] = df['cum_inc']/(df.groupby('website-index').cumcount()+1)\n",
    "    df['perc_out'] =  df['cum_out']/(df.groupby('website-index').cumcount()+1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous 100 packet sizes as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_packets(df, prev=False):\n",
    "    if (prev):\n",
    "        df = cumulative_packets(df, True)\n",
    "    \n",
    "    # Get 100 previous packet sizes --> we can make it p_i instead of packet_size too\n",
    "    for i in range(1, 101):\n",
    "        name = \"prev_\"+str(i)\n",
    "\n",
    "        if (prev):\n",
    "            df[name] = df.groupby('website-index')['p_i'].shift(i)\n",
    "        else:\n",
    "            df[name] = df.groupby('website-index')['packet size'].shift(i)\n",
    "        \n",
    "    # Remove NaN\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total discrete statistics \n",
    "\n",
    "Number of packet statistics, such as packet count per website, etc. Denoted in Hayes 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(q, r):\n",
    "    if r == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return q / r\n",
    "\n",
    "def n_o_statistics(df, prev=False):\n",
    "    if (prev):\n",
    "        df = previous_packets(df, True)\n",
    "    \n",
    "    # Total number of packets per session, total number of incoming/outgoing packets + percentages\n",
    "    curr_index = df.at[0, 'website-index']\n",
    "    total_inc_packets = 0\n",
    "    total_out_packets = 0\n",
    "    inc_values = []\n",
    "    out_values = []\n",
    "    total_values = []\n",
    "    total_incoming_percentage = []\n",
    "    total_outgoing_percentage = []\n",
    "    for index, row in df.iterrows():\n",
    "        if (row['website-index'] != curr_index):\n",
    "            total_packets = total_inc_packets + total_out_packets\n",
    "            \n",
    "            inc_values.extend([total_inc_packets] * total_packets)\n",
    "            out_values.extend([total_out_packets] * total_packets)\n",
    "            total_values.extend([total_packets] * total_packets)\n",
    "            \n",
    "            total_incoming_percentage.extend([safe_div(total_inc_packets, total_packets)] * (total_packets))\n",
    "            total_outgoing_percentage.extend([safe_div(total_out_packets, total_packets)] * (total_packets))\n",
    "            curr_index = row['website-index']\n",
    "            \n",
    "            if row['direction'] == 0:\n",
    "                total_inc_packets = 1\n",
    "                total_out_packets = 0\n",
    "            else:\n",
    "                total_inc_packets = 0\n",
    "                total_out_packets = 1\n",
    "        else: \n",
    "            if row['direction'] == 0:\n",
    "                total_inc_packets += 1\n",
    "            else:\n",
    "                total_out_packets += 1\n",
    "    # Repeat for last website index\n",
    "    total_packets = total_inc_packets + total_out_packets\n",
    "    inc_values.extend([total_inc_packets] * total_packets)\n",
    "    out_values.extend([total_out_packets] * total_packets)\n",
    "    total_values.extend([total_packets] * (total_packets))\n",
    "    total_incoming_percentage.extend([safe_div(total_inc_packets, total_packets)] * (total_packets))\n",
    "    total_outgoing_percentage.extend([safe_div(total_out_packets, total_packets)] * (total_packets))\n",
    "\n",
    "    df['total_incoming'] = inc_values\n",
    "    df['total_outgoing'] = out_values\n",
    "    df['total_packets'] = total_values\n",
    "    df['total_incoming_percentage'] = total_incoming_percentage\n",
    "    df['total_outgoing_percentage'] = total_outgoing_percentage\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packet ordering statistics hayes 2016\n",
    "For each successive incoming and outgoing packet, the total number of packets seen before it in the sequence\n",
    "\n",
    "The standard deviation of the outgoing packet ordering list\n",
    "\n",
    "The standard deviation of the incoming packet ordering list \n",
    "\n",
    "The average of the incoming packet ordering list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering_statistics(df, prev=False):\n",
    "    if (prev):\n",
    "        df = n_o_statistics(df, True)\n",
    "    \n",
    "    # For each successive incoming and outgoing packet, the total number of packets seen before it in the sequence\n",
    "    \"\"\"\n",
    "    e.g. \n",
    "    direction successive_num_packets\n",
    "    0 0 \n",
    "    0 1\n",
    "    0 2\n",
    "    1 0 \n",
    "    0 0\n",
    "    0 1\n",
    "    \"\"\"\n",
    "    total_seen = 0\n",
    "    last_dir = -1\n",
    "    result = []\n",
    "    curr_index = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        if row['website-index'] != curr_index:\n",
    "            total_seen = 0\n",
    "            last_dir = -1\n",
    "            curr_index = row['website-index']\n",
    "        if row.direction == last_dir:\n",
    "            total_seen += 1\n",
    "            result.append(total_seen)\n",
    "        else:\n",
    "            total_seen = 0\n",
    "            result.append(total_seen)\n",
    "            last_dir = row.direction \n",
    "    df['successive_num_packets'] = result\n",
    "    \n",
    "    # The average and standard deviation of the in/outgoing packet ordering list\n",
    "    curr_index = df.at[0, 'website-index']\n",
    "    outgoing_vals = []\n",
    "    incoming_vals = []\n",
    "    incoming_order_std = []\n",
    "    outgoing_order_std = []\n",
    "    outgoing_order_avg = []\n",
    "    incoming_order_avg = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if (row['website-index'] != curr_index):\n",
    "            # Get std of in/outgoing_vals and reset\n",
    "            curr_index = row['website-index']\n",
    "            out_std = np.std(outgoing_vals)\n",
    "            in_std = np.std(incoming_vals)\n",
    "            out_avg = np.mean(outgoing_vals)\n",
    "            in_avg = np.mean(incoming_vals)\n",
    "            total_segment_length = len(outgoing_vals) + len(incoming_vals)\n",
    "            incoming_order_std.extend([in_std] * total_segment_length)\n",
    "            outgoing_order_std.extend([out_std] * total_segment_length)\n",
    "            outgoing_order_avg.extend([out_avg] * total_segment_length)\n",
    "            incoming_order_avg.extend([in_avg] * total_segment_length)\n",
    "            \n",
    "            if row.direction == 1:\n",
    "                outgoing_vals = [row['successive_num_packets']]\n",
    "                incoming_vals = []\n",
    "            else: \n",
    "                incoming_vals = [(row['successive_num_packets']) ]\n",
    "                outgoing_vals = []\n",
    "\n",
    "        else:\n",
    "            if row.direction == 1:\n",
    "                outgoing_vals.append(row['successive_num_packets'])\n",
    "            else: \n",
    "                incoming_vals.append(row['successive_num_packets']) \n",
    "                \n",
    "    # Repeat for last segment \n",
    "    out_std = np.std(outgoing_vals)\n",
    "    in_std = np.std(incoming_vals)\n",
    "    out_avg = np.mean(outgoing_vals)\n",
    "    in_avg = np.mean(incoming_vals)\n",
    "    total_segment_length = len(outgoing_vals) + len(incoming_vals)\n",
    "    incoming_order_std.extend([in_std] * total_segment_length)\n",
    "    outgoing_order_std.extend([out_std] * total_segment_length)\n",
    "    outgoing_order_avg.extend([out_avg] * total_segment_length)\n",
    "    incoming_order_avg.extend([in_avg] * total_segment_length)\n",
    "    \n",
    "    \n",
    "    df['outgoing_order_std'] = outgoing_order_std\n",
    "    df['incoming_order_std'] = incoming_order_std\n",
    "    df['outgoing_order_avg'] = outgoing_order_avg\n",
    "    df['incoming_order_avg'] = incoming_order_avg\n",
    "\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concentration of incoming & outgoing packets in first & last 30 packets. Hayes 2016\n",
    "\n",
    "The number of incoming and outgoing packets in the first and last 30 packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_30(df, prev=False):\n",
    "    if (prev):\n",
    "        df = ordering_statistics(df, True)\n",
    "        \n",
    "    # Number of incoming/outgoing packets in the first/last 30 packets \n",
    "    curr_index = df.at[0, 'website-index']\n",
    "    num_inc_first_30_vals = []\n",
    "    num_out_first_30_vals = []\n",
    "    num_inc_last_30_vals = []\n",
    "    num_out_last_30_vals = []\n",
    "    dirs = []\n",
    "    # Loop through each segment, grab direction column, take first 30, last 30, sum for in/out \n",
    "    for index, row in df.iterrows():\n",
    "        if (row['website-index'] != curr_index):\n",
    "            curr_index = row['website-index']\n",
    "            # Sum up dirs\n",
    "            ins_first_30 = len(list(filter(lambda d: d == 0, dirs[:30])))\n",
    "            ins_last_30 = len(list(filter(lambda d: d == 0, dirs[-30:])))\n",
    "            out_first_30 = len(list(filter(lambda d: d == 1, dirs[:30])))\n",
    "            out_last_30 = len(list(filter(lambda d: d == 1, dirs[-30:])))\n",
    "            num_packets = len(dirs)\n",
    "            num_inc_first_30_vals.extend([ins_first_30] * num_packets)\n",
    "            num_out_first_30_vals.extend([out_first_30] * num_packets)\n",
    "            num_inc_last_30_vals.extend([ins_last_30] * num_packets)\n",
    "            num_out_last_30_vals.extend([out_last_30] * num_packets)\n",
    "\n",
    "            dirs = [row['direction']]\n",
    "        else:\n",
    "            dirs.append(row['direction'])\n",
    "\n",
    "    ins_first_30 = len(list(filter(lambda d: d == 0, dirs[:30])))\n",
    "    ins_last_30 = len(list(filter(lambda d: d == 0, dirs[-30:])))\n",
    "    out_first_30 = len(list(filter(lambda d: d == 1, dirs[:30])))\n",
    "    out_last_30 = len(list(filter(lambda d: d == 1, dirs[-30:])))\n",
    "    num_packets = len(dirs)\n",
    "    num_inc_first_30_vals.extend([ins_first_30] * num_packets)\n",
    "    num_out_first_30_vals.extend([out_first_30] * num_packets)\n",
    "    num_inc_last_30_vals.extend([ins_last_30] * num_packets)\n",
    "    num_out_last_30_vals.extend([out_last_30] * num_packets)\n",
    "\n",
    "    df['num_inc_first_30_vals'] = num_inc_first_30_vals\n",
    "    df['num_out_first_30_vals'] = num_out_first_30_vals\n",
    "    df['num_inc_last_30_vals'] = num_inc_last_30_vals\n",
    "    df['num_out_last_30_vals'] = num_out_last_30_vals\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concentration of outgoing packets\n",
    "\n",
    "The packet sequence split into non-overlapping chunks of 20 packets, where we count the number of outgoing packets in each chunk. Then extract the standard deviation and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(df):\n",
    "    ret = []\n",
    "    start = 0\n",
    "    curr_index = df.at[0, 'website-index']\n",
    "    for index, row in df.iterrows():\n",
    "        if (row['website-index'] != curr_index):\n",
    "            curr_index = row['website-index']\n",
    "            ret.append((start, index - 1))\n",
    "            start = index\n",
    "            \n",
    "    ret.append((start, index))\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_20s_from_segments(segments):\n",
    "    # Given an array of start:end tuples, return an array of start2:end2 tuples where end2-start2 == 20, or less if not possible\n",
    "    # e.g. [(0,45)] => [(0, 19), (20, 39), (40, 45)]\n",
    "    ret = []\n",
    "    for s in segments:\n",
    "        start = s[0]\n",
    "        end = s[1]\n",
    "        i = start\n",
    "        while i + 19 <= end:\n",
    "            ret.append((i, i + 19))\n",
    "            i += 20\n",
    "        ret.append((i, end))\n",
    "    return ret       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outgoing_concentration(df, prev=False):\n",
    "    if (prev):\n",
    "        df = first_last_30(df, True)\n",
    "        \n",
    "    # Number of outgoing packets in each chunk of 20 packets per segment\n",
    "    # get an array of indexes for each segment \n",
    "    segments = get_segments(df)\n",
    "    segment_20s = get_20s_from_segments(segments)    \n",
    "    num_outgoing_vals = [] # Don't add this to the data, we only need it to collect statistics\n",
    "#     debug = []\n",
    "    for s in segment_20s:\n",
    "        start = s[0]\n",
    "        end = s[1]\n",
    "        num_outs = df.iloc[start:end+1][df.direction == 1].shape[0]\n",
    "        num_outgoing_vals.append(num_outs)\n",
    "#         debug.extend([num_outs] * (end - start + 1))\n",
    "\n",
    "        \n",
    "    # We want to get the standard deviation, mean of the num_outgoing_vals for each segment\n",
    "    # For each segment in segments, we want to obtain the corresponding values in num_outgoing_vals \n",
    "    # which are segmentized by 20s\n",
    "    outgoing_20_std = []\n",
    "    outgoing_20_avg = []\n",
    "\n",
    "    for segment in segments:\n",
    "        start = segment[0]\n",
    "        end = segment[1]\n",
    "        values = []\n",
    "        for i in range(len(segment_20s)):\n",
    "            seg20 = segment_20s[i]\n",
    "            if seg20[0] < start:\n",
    "                continue\n",
    "            if seg20[0] > end:\n",
    "                break\n",
    "            values.append(num_outgoing_vals[i])\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        outgoing_20_avg.extend([mean] * (end - start + 1))\n",
    "        outgoing_20_std.extend([std] * (end - start + 1))\n",
    "\n",
    "    df['outgoing_20_avg'] = outgoing_20_avg\n",
    "    df['outgoing_20_std'] = outgoing_20_std\n",
    "#     df['debug'] = debug\n",
    "\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df_one, df_two, df_three):\n",
    "    # Preprocess training data\n",
    "    df = df_one.append(df_two)\n",
    "    X = df.loc[:, df.columns != 'website-index']\n",
    "    y = df['website-index']\n",
    "\n",
    "    # Preprocess testing data\n",
    "    X_test = df_three.loc[:, df_three.columns != 'website-index']\n",
    "    y_test = df_three['website-index']\n",
    "\n",
    "    # Model training\n",
    "    clf = RandomForestClassifier(n_estimators=300, n_jobs=-1 )\n",
    "    # Change n_estimators to what you want (defualt 100)\n",
    "    # n_jobs=-1 will run the classifier on ALL cpu's, lower this if running on bad comp\n",
    "    \n",
    "    clf.fit(X,y)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=20)\n",
    "    knn.fit(X,y)\n",
    "\n",
    "    # Evaluate\n",
    "    rf_acc = clf.score(X_test, y_test)\n",
    "    knn_acc = knn.score(X_test, y_test)\n",
    "\n",
    "    print(\"RF acc: \" + str(rf_acc))\n",
    "    print(\"KNN acc: \" + str(knn_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
