{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os \n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community data model\n",
    "\n",
    "Because of the absolute time issue, this model's performance is not representative or meaningful. It is included mainly for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF acc: 0.9708700651590648\n",
      "SVM acc: 0.9862016098121886\n"
     ]
    }
   ],
   "source": [
    "# Community data import (make sure to have path with access to parsed community data)\n",
    "path=\"../../p2/parsed/community/\"\n",
    "day_one=\"day_1\"\n",
    "day_two=\"day_2\"\n",
    "day_three=\"day_3\"\n",
    "file=\"_parsed.csv\"\n",
    "col_names = [\"website-index\", \"time\", \"direction\", \"packet size\"]\n",
    "\n",
    "df_one = pd.read_csv(path+day_one+file, header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False)\n",
    "df_two = pd.read_csv(path+day_two+file, header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False)\n",
    "df_three = pd.read_csv(path+day_three+file, header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "# Preprocess training data\n",
    "df = df_one.append(df_two)\n",
    "X = df.loc[:, df.columns != 'website-index']\n",
    "y = df['website-index']\n",
    "\n",
    "# Preprocess testing data\n",
    "X_test = df_three.loc[:, df_three.columns != 'website-index']\n",
    "y_test = df_three['website-index']\n",
    "\n",
    "# Model training\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X,y)\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "rf_acc = clf.score(X_test, y_test)\n",
    "svm_acc = svm_model.score(X_test, y_test)\n",
    "\n",
    "print(\"RF acc: \" + str(rf_acc))\n",
    "print(\"SVM acc: \" + str(svm_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data (all possible protocol formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF acc: 0.20582571253629342\n",
      "{'all': {'rf': '0.20582571253629342'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# We want to loop over the folders in /collection\n",
    "# load up the dataframes in each of them\n",
    "# Train the model\n",
    "# Dump it out \n",
    "\n",
    "path = '../../p2/collection'\n",
    "col_names = [\"website-index\", \"time\", \"direction\", \"packet size\"]\n",
    "folders = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for folder in d:\n",
    "        folders.append(folder)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for folder in folders:\n",
    "    if (folder == \"raw\"):\n",
    "        continue \n",
    "    results[folder] = {} # Goddam python dictionaries\n",
    "    print(folder)\n",
    "    df_one = pd.read_csv(path+\"/\"+folder+\"/\"+\"Day1-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "    df_two = pd.read_csv(path+\"/\"+folder+\"/\"+\"Day2-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "    df_three = pd.read_csv(path+\"/\"+folder+\"/\"+\"Day3-parsed-ondevice.csv\", header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "    \n",
    "    # Preprocess training data\n",
    "    df = df_one.append(df_two)\n",
    "    X = df.loc[:, df.columns != 'website-index']\n",
    "    y = df['website-index']\n",
    "    \n",
    "    # Preprocess testing data\n",
    "    X_test = df_three.loc[:, df_three.columns != 'website-index']\n",
    "    y_test = df_three['website-index']\n",
    "    \n",
    "    outpath = './models/' + folder + \"/\"\n",
    "    if not os.path.exists(outpath):\n",
    "        os.mkdir(outpath)    \n",
    "\n",
    "    # Model training\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X,y)\n",
    "    rf_acc = clf.score(X_test, y_test)\n",
    "    print(\"RF acc: \" + str(rf_acc))\n",
    "    results[folder][\"rf\"] = str(rf_acc)\n",
    "    dump(clf, outpath + 'rf_model.joblib')\n",
    "    print(results)\n",
    "\n",
    "    \n",
    "    \n",
    "    svm_model = svm.SVC(verbose=True)\n",
    "    svm_model.fit(X, y)\n",
    "    svm_acc = svm_model.score(X_test, y_test)\n",
    "    print(\"SVM acc: \" + str(svm_acc))\n",
    "    results[folder][\"svm\"] = str(svm_acc)\n",
    "    dump(svm_model, outpath + 'svm_model.joblib')\n",
    "    print(results)\n",
    "\n",
    "    \n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=10e5)\n",
    "    lr.fit(X, y)\n",
    "    lr_acc = lr.score(X_test, y_test)\n",
    "    print(\"Logistic Reg acc: \" + str(lr_acc))\n",
    "    results[folder][\"lr\"] = str(lr_acc)\n",
    "    dump(lr, outpath + \"lr_model.joblib\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] So we want the output of all the combinations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newly collected data\n",
    "\n",
    "After receiving the community scripts, we wanted to compare out original locally collected data against the data we would collect using the community scripts. So we corrected and adapted the scripts (because of the absolute time issue). As you can see down below, the performance of the model (both SVM & RF) is much worse; basically equal to random guessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./community_script/aggregate_data/\"\n",
    "day_one=\"day_1\"\n",
    "day_two=\"day_2\"\n",
    "day_three=\"day_3\"\n",
    "file=\"_parsed.csv\"\n",
    "col_names = [\"website-index\", \"time\", \"direction\", \"packet size\"]\n",
    "\n",
    "\n",
    "df_one = pd.read_csv(path+day_one+file, header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "df_two = pd.read_csv(path+day_two+file, header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')\n",
    "df_three = pd.read_csv(path+day_three+file, header=0, names=col_names, error_bad_lines=False, warn_bad_lines=False, quotechar='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website-index</th>\n",
       "      <th>time</th>\n",
       "      <th>direction</th>\n",
       "      <th>packet size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube</td>\n",
       "      <td>4.241315</td>\n",
       "      <td>1</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube</td>\n",
       "      <td>4.254662</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube</td>\n",
       "      <td>4.275345</td>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>4.289213</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yahoo</td>\n",
       "      <td>3.834408</td>\n",
       "      <td>1</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>imgur</td>\n",
       "      <td>3.842293</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>dropbox</td>\n",
       "      <td>3.527019</td>\n",
       "      <td>1</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>dropbox</td>\n",
       "      <td>3.537343</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>dropbox</td>\n",
       "      <td>3.556095</td>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>dropbox</td>\n",
       "      <td>3.566647</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    website-index      time  direction  packet size\n",
       "0         youtube  4.241315          1          368\n",
       "1         youtube  4.254662          0          450\n",
       "2         youtube  4.275345          1          373\n",
       "3         youtube  4.289213          0          450\n",
       "4           yahoo  3.834408          1          368\n",
       "..            ...       ...        ...          ...\n",
       "804         imgur  3.842293          0          450\n",
       "805       dropbox  3.527019          1          368\n",
       "806       dropbox  3.537343          0          450\n",
       "807       dropbox  3.556095          1          373\n",
       "808       dropbox  3.566647          0          450\n",
       "\n",
       "[809 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data correctness:\n",
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF acc: 0.051597051597051594\n",
      "LR acc: 0.056511056511056514\n",
      "SVM acc: 0.05405405405405406\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training data\n",
    "df = df_one.append(df_two)\n",
    "X = df.loc[:, df.columns != 'website-index']\n",
    "y = df['website-index']\n",
    "\n",
    "# Preprocess testing data\n",
    "X_test = df_three.loc[:, df_three.columns != 'website-index']\n",
    "y_test = df_three['website-index']\n",
    "\n",
    "# Model training\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X,y)\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X, y)\n",
    "\n",
    "lr = LogisticRegression(random_state=0, max_iter=10e5)\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "rf_acc = clf.score(X_test, y_test)\n",
    "svm_acc = svm_model.score(X_test, y_test)\n",
    "lr_acc = lr.score(X_test, y_test)\n",
    "\n",
    "# print(\"Logistic Reg acc: \" + str(lr_acc))\n",
    "print(\"RF acc: \" + str(rf_acc))\n",
    "print(\"LR acc: \" + str(lr_acc))\n",
    "print(\"SVM acc: \" + str(svm_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the performance on the newly collected data using the community scripts is much worse, we continue to model on the originally collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
